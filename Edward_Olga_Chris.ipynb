{"cells":[{"metadata":{},"cell_type":"markdown","source":"![image](https://www.livetradingnews.com/wp-content/uploads/2017/01/home-sales-701x526.jpg)\n<div style=\"text-align: center\" >Machine Learning Workflow of House Prices </div>\n\n<div style=\"text-align: center\"> Being a part of Kaggle gives us unlimited access to learn, share and grow as a Data Scientist. In this kernel, we want to solve <font color=\"red\"><b>House Sales Prices with Advanced Regression Analysis</b></font>. We're going to share how we work with a dataset step by step  <b>from data preparation and data analysis to statistical tests and implementing machine learning models.</b> We'll also describe the model results along with many other tips. Let's get started.</div>\n\n<div style=\"text-align:center\"> If there are any recommendations/changes you would like to see in this notebook, please <b>leave a comment</b>. Any feedback/constructive criticism would be genuinely appreciated. \n "},{"metadata":{},"cell_type":"markdown","source":"# Goals\nThis kernel hopes to accomplish a few goals;\n* Do a comprehensive data analysis along with visualizations. \n* Create models that are well equipped to predict better sale price of the houses. \n\n# Introduction\nThis kernel goes on a detailed analysis journey of most of the regression algorithms.  In addition to that, this kernel uses many charts and images to make things easier for readers to understand.\n# Importing Necessary Libraries and datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.gridspec as gridspec\nfrom datetime import datetime\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nimport matplotlib.style as style\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read in both the Train and Test datasets\ntrain = pd.read_csv('/kaggle/input/group-assignment-ace-2020/train.csv')\ntest = pd.read_csv('/kaggle/input/group-assignment-ace-2020/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A Glimpse of the datasets.\n> **Sample Train Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ** Sample Test Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Taking Description of the Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train set size:\", train.shape)\nprint(\"Test set size:\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see statistical information about the numerical variables. \ntrain.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see information about the features. \ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What's the count of different types of objects.\n\ntrain.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for Missing Values"},{"metadata":{},"cell_type":"markdown","source":"### Missing Train values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    ## the two following line may seem complicated but its actually very simple. \n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n    percent = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n\nmissing_percentage(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Test values"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_percentage(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observation\n* There are multiple types of features. \n* Some features have missing values. \n* Most of the features are object( includes string values in the variable).\n\nLet's focus on the target variable which is **SalePrice.** Let's create a histogram to see if the target variable is Normally distributed. If we want to create any linear model, it is essential that the features are normally distributed. This is one of the assumptions of multiple linear regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotting_3_chart(df, feature):\n    ## Importing seaborn, matplotlab and scipy modules. \n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import matplotlib.gridspec as gridspec\n    from scipy import stats\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(15,10))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n    \nplotting_3_chart(train, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These **three** charts above can tell us a lot about our target variable.\n* Our target variable, **SalePrice** is not normally distributed.\n* Our target variable is right-skewed. \n* There are multiple outliers in the variable. \n\n\nLet's find out how the sales price is distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"#skewness and kurtosis\nprint(\"Skewness: \" + str(train['SalePrice'].skew()))\nprint(\"Kurtosis: \" + str(train['SalePrice'].kurt()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like there are quite a bit Skewness and Kurtosis in the target variable. Let's talk about those a bit. \n\n<b>Skewness</b> \n* is the degree of distortion from the symmetrical bell curve or the normal curve. \n* So, a symmetrical distribution will have a skewness of \"0\". \n* There are two types of Skewness: <b>Positive and Negative.</b> \n* <b>Positive Skewness</b>(similar to our target variable distribution) means the tail on the right side of the distribution is longer and fatter. \n* In <b>positive Skewness </b> the mean and median will be greater than the mode. This is similar to this dataset. So, in Layman's terms, more houses were sold by less than the average price. \n* <b>Negative Skewness</b> means the tail on the left side of the distribution is longer and fatter.\n* In <b>negative Skewness </b> the mean and median will be less than the mode. \n* Skewness differentiates extreme values in one versus the other tail. \n\nHere is a picture to make more sense.  \n![image](https://cdn-images-1.medium.com/max/1600/1*nj-Ch3AUFmkd0JUSOW_bTQ.jpeg)\n\n\n<b>Kurtosis</b>\nAccording to Wikipedia, \n\n*In probability theory and statistics, **Kurtosis** is the measure of the \"tailedness\" of the probability. distribution of a real-valued random variable.* So, In other words it is the measure of the extreme values(outliers) present in the distribution. \n\n* There are three types of Kurtosis: <b>Mesokurtic, Leptokurtic and Platykurtic</b>. \n* Mesokurtic is similar to normal curve with the standard value of 3. This means that the extreme values of this distrubution is similar to that of a normal distribution. \n* Leptokurtic Example of leptokurtic distributions are the T-distributions with small degrees of freedom.\n* Platykurtic: Platykurtic describes a particular statistical distribution with thinner tails than a normal distribution. Because this distribution has thin tails, it has fewer outliers (e.g., extreme values three or more standard deviations from the mean) than do mesokurtic and leptokurtic distributions. \n\n![image](https://i2.wp.com/mvpprograms.com/help/images/KurtosisPict.jpg?resize=375%2C234)\n\n\nYou can read more about this from [this](https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa) article. \n\nWe can fix this by using different types of transformation,  However, before doing that, let's find out the relationships among the target variable and other predictor variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Getting the correlation of all the features with target variable. \n(train.corr()**2)[\"SalePrice\"].sort_values(ascending = False)[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the predictor variables sorted in a descending order starting with the most correlated one **OverallQual**. Let's put this one in a scatter plot and see how it looks."},{"metadata":{},"cell_type":"markdown","source":"### SalePrice vs OverallQual"},{"metadata":{"trusted":true},"cell_type":"code","source":"def customized_scatterplot(y, x):\n        ## Sizing the plot. \n    style.use('fivethirtyeight')\n    plt.subplots(figsize = (15,10))\n    ## Plotting target variable with predictor variable(OverallQual)\n    sns.scatterplot(y = y, x = x);\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.SalePrice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"customized_scatterplot(train.SalePrice, train.OverallQual)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, **OverallQual** is a categorical variable and scatter plot is not the best way to visualize categorical variables. However, it looks like some of the houses are overpriced compared to their overall quality. These could be outliers. Let's check out some more features to determine the outliers. Let's focus on the numerical variables this time."},{"metadata":{},"cell_type":"markdown","source":"### SalePrice vs GrLivArea"},{"metadata":{"trusted":true},"cell_type":"code","source":"customized_scatterplot(train.SalePrice, train.GrLivArea)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see there are two outliers in the plot above. We will get rid off them later. Let's look at another scatter plot with a different feature.\n\n### SalePrice vs GarageArea"},{"metadata":{"trusted":true},"cell_type":"code","source":"customized_scatterplot(train.SalePrice, train.GarageArea);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the next one..?\n### SalePrice vs TotalBsmtSF"},{"metadata":{"trusted":true},"cell_type":"code","source":"customized_scatterplot(train.SalePrice, train.TotalBsmtSF)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and the next ?\n### SalePrice vs 1stFlrSF"},{"metadata":{"trusted":true},"cell_type":"code","source":"customized_scatterplot(train.SalePrice, train['1stFlrSF']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How about one more...\n\n### SalePrice vs MasVnrArea"},{"metadata":{"trusted":true},"cell_type":"code","source":"customized_scatterplot(train.SalePrice, train.MasVnrArea);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, I think we have seen enough. Let's discuss what we have found so far. \n\n# Observations\n* Our target variable shows an unequal level of variance across most predictor(independent) variable values. This is called **Heteroscedasticity(more explanation below)**; and is a red flag for the multiple linear regression model.\n* There are many outliers in the scatter plots above that took our attention. \n\n* The two next to the top right edge of **SalePrice vs. GrLivArea** seems to follow a trend, which can be explained by saying that \"As the prices increased so did the area. \n* However, The two on the bottom right of the same chart do not follow any trends. We will get rid of these two below."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Deleting those two values with outliers. \ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop = True, inplace = True)\n\n## save a copy of this dataset so that any changes later on can be compared side by side.\nprevious_train = train.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we look through these scatter plots, We realized that it is time to explain the assumptions of Multiple Linear Regression. Before building a multiple linear regression model, we need to check that these assumptions below are valid.\n## Assumptions of Regression\n\n* **Linearity ( Correct functional form )** \n* **Homoscedasticity ( Constant Error Variance )( vs Heteroscedasticity ). **\n* **Independence of Errors ( vs Autocorrelation ) **\n* **Multivariate Normality ( Normality of Errors ) **\n* **No or little Multicollinearity. ** \n\nSince we fit a linear model, we assume that the relationship is **linear** and the errors, or residuals, are pure random fluctuations around the true line. We assume that the variability in the dependent variable doesn't increase as the value of the predictor(independent) increases, which is the assumption of equal variance, also known as **Homoscedasticity**. We also assume that the observations are independent of one another(**No Multicollinearity**) and a correlation between sequential observations or auto-correlation are not there.\n\nNow, these assumptions are prone to happen altogether. In other words, if we see one of these assumptions in the dataset, it's more likely that we may come across with one of the different assumptions. Similarly, we can find and fix various assumptions with a few unique techniques.\n\nSo, **How do we check regression assumptions? We fit a regression line and look for the variability of the response data along the regression line.** Let's apply this for each one of them.\n\n**Linearity ( Correct functional form )** \nLinear regression needs the relationship between each independent variable and the dependent variable to be linear. The linearity assumption can be tested with scatter plots. The following two examples depict two cases, where no or little linearity is present. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot sizing. \nfig, (ax1, ax2) = plt.subplots(figsize = (20,10), ncols=2,sharey=False)\n## Scatter plotting for SalePrice and GrLivArea. \nsns.scatterplot( x = train.GrLivArea, y = train.SalePrice,  ax=ax1)\n## Putting a regression line. \nsns.regplot(x=train.GrLivArea, y=train.SalePrice, ax=ax1)\n\n## Scatter plotting for SalePrice and MasVnrArea. \nsns.scatterplot(x = train.MasVnrArea,y = train.SalePrice, ax=ax2)\n## regression line for MasVnrArea and SalePrice. \nsns.regplot(x=train.MasVnrArea, y=train.SalePrice, ax=ax2);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are plotting our target variable with two independent variables **GrLivArea** and **MasVnrArea**. It's pretty apparent from the chart that there is a better linear relationship between **SalePrice** and **GrLivArea** than **SalePrice** and **MasVnrArea**. One thing to take note here, there are some outliers in the dataset. It is imperative to check for outliers since linear regression is sensitive to outlier effects. Sometimes we may be trying to fit a linear regression model when the data might not be so linear, or the function may need another degree of freedom to fit the data. In that case, we may need to change our function depending on the data to get the best possible fit. In addition to that, we can also check the residual plot, which tells us how is the error variance across the true line. Let's look at the residual plot for independent variable **GrLivArea** and our target variable **SalePrice **."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize = (15,10))\nsns.residplot(train.GrLivArea, train.SalePrice);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ideally, if the assumptions are met, the residuals will be randomly scattered around the centerline of zero with no apparent pattern. The residual will look like an unstructured cloud of points centered around zero. However, our residual plot is anything but an unstructured cloud of points. Even though it seems like there is a linear relationship between the response variable and predictor variable, the residual plot looks more like a funnel. The error plot shows that as **GrLivArea** value increases the variance also increases, which is the characteristics known as **Heteroscedasticity**; another assumption of linear regression. Let's break this down. \n\n**Homoscedasticity ( Constant Variance ):** \nThe assumption of Homoscedasticity is crucial to linear regression models. Homoscedasticity describes a situation in which the error term or variance or the \"noise\" or random disturbance in the relationship between the independent variables and the dependent variable is same across all values of the independent variable. In other words, there is a constant variance present in the response variable as the predictor variable increases. If the \"noise\" is not the same across the values of an independent variable like the residual plot above, we call that **Heteroscedasticity**. As you can tell, it is the opposite of **Homoscedasticity.**\n\n<p><img src=\"https://www.dummies.com/wp-content/uploads/415147.image1.jpg\" style=\"float:center\"></img></p>\n\nThis plot above is an excellent example of Homoscedasticity. As you can see, the residual variance is the same as the value of the predictor variable increases. One way to fix this Heteroscedasticity is by using a transformation method like log-transformation or box-cox transformation. We will do that later.\n\n**Multivariate Normality ( Normality of Errors):**\nThe linear regression analysis requires the dependent variable to be multivariate normally distributed. A histogram or a Q-Q-Plot can check whether the target variable is normally distributed or not. The goodness of fit test, e.g., the Kolmogorov-Smirnov test or can check for normality in the dependent variable. We already know that our target variable does not follow a normal distribution. Let's bring back the three charts to show our target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting_3_chart(train, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's make sure that the target variable follows a normal distribution. If you want to learn more about probability plot(Q-Q plot), try [this](https://www.youtube.com/watch?v=smJBsZ4YQZw) video.."},{"metadata":{"trusted":true},"cell_type":"code","source":"## trainsforming target variable using numpy.log1p, \ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n## Plotting the newly transformed response variable\nplotting_3_chart(train, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see the log transformation removes the normality of errors. This solves some of the other assumptions that we talked about above like Homoscedasticity. Let's make a comparison of the pre-transformed and post-transformed state of residual plots. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Customizing grid for two plots. \nfig, (ax1, ax2) = plt.subplots(figsize = (20,6), ncols=2, sharey = False, sharex=False)\n## doing the first scatter plot. \nsns.residplot(x = previous_train.GrLivArea, y = previous_train.SalePrice, ax = ax1)\n## doing the scatter plot for GrLivArea and SalePrice. \nsns.residplot(x = train.GrLivArea, y = train.SalePrice, ax = ax2);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see that the pre-transformed chart on the left has heteroscedasticity, and the post-transformed chart on the right has almost an equal amount of variance across the zero lines.\n\n**No or Little multicollinearity:** \nMulticollinearity is when there is a strong correlation between independent variables. Linear regression or multilinear regression requires independent variables to have little or no similar features. Multicollinearity can lead to a variety of problems, including:\n* The effect of predictor variables estimated by our regression will depend on what other variables are included in our model. \n* Predictors can have wildly different results depending on the observations in our sample, and small changes in samples can result in very different estimated effects. \n* With very high multicollinearity, the inverse matrix, the computer calculates may not be accurate. \n* We can no longer interpret a coefficient on a variable as the effect on the target of a one-unit increase in that variable holding the other variables constant. The reason behind that is, when predictors are strongly correlated, there is not a scenario in which one variable can change without a conditional change in another variable.\n\nHeatmap is an excellent way to identify whether there is multicollinearity or not. The best way to solve multicollinearity is to use regularization methods like Ridge or Lasso."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot fig sizing. \nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train.corr(), cmap=sns.diverging_palette(20, 220, n=200), mask = mask, annot=True, center = 0, );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation. \nAs we can see, the multicollinearity still exists in various features. However, we will keep them for now for the sake of learning and let models(Regularization models such as Lasso, Ridge) do the clean up later on. Let's go through some of the correlations that still exists. \n\n* There is 0.83 or 83% correlation between **GarageYrBlt** and **YearBuilt**. \n* 83% correlation between **TotRmsAbvGrd ** and **GrLivArea**. \n* 89% correlation between **GarageCars** and **GarageArea**. \n* Similarly many other features such as**BsmtUnfSF**, **FullBath** have good correlation with other independent feature but not so much with the dependent feature.\n\nIf we were using only multiple linear regression, deleting these features from the dataset would fit multiple linear regression algorithms better. However, we will be using many algorithms as scikit learn modules makes it easy to implement them and get the best possible model. Therefore, we will keep all the features for now. \n\n\n# Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Dropping the \"Id\" from train and test set. \n# train.drop(columns=['Id'],axis=1, inplace=True)\n\ntrain.drop(columns=['Id'],axis=1, inplace=True)\ntest.drop(columns=['Id'],axis=1, inplace=True)\n\n## Saving the target values in \"y_train\". \ny = train['SalePrice'].reset_index(drop=True)\n\n\n\n# getting a copy of train\nprevious_train = train.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concatenation###\nTo keep consistency between test and train features we concatenate the two sets while remembering the index so we can split it later again."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Combining train and test datasets together so that we can do all the work at once. \nfeatures = pd.concat((train, test)).reset_index(drop = True)\n\n## Dropping the target variable. \nfeatures.drop(['SalePrice'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Missing Values\n> **Missing data in train and test data(all_data)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_percentage(features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Imputing Missing Values (NA's)##\nLet's figure out what NA's exist, sort them by categories and impute them in the best possible way."},{"metadata":{"trusted":true},"cell_type":"code","source":"nulls = np.sum(features.isnull())\nnullcols = nulls.loc[(nulls != 0)]\ndtypes = features.dtypes\ndtypes2 = dtypes.loc[(nulls != 0)]\ninfo = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\nprint(info)\nprint(\"There are\", len(nullcols), \"columns with missing values\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of these can be filled with 'None'. Some exceptions though:\n\n* Functional: The documentation says that we should assume \"Typ\", so lets impute that.\n* Electrical: The documentation doesn't give any information but obviously every house has this so let's impute the most common value: \"SBrkr\".\n* KitchenQual: Similar to Electrical, most common value: \"TA\".\n* Exterior 1 and Exterior 2: Let's use the most common one here.\n* SaleType: Similar to electrical, let's use most common value."},{"metadata":{"trusted":true},"cell_type":"code","source":"features['Functional'] = features['Functional'].fillna('Typ')\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check some points individually to figure out the best imputation strategy"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('max_columns', None)\nfeatures[features['PoolArea'] > 0 & features['PoolQC'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are three NaN's foor PoolQC that have a PoolArea. Let's impute them based on overall quality of the house."},{"metadata":{"trusted":true},"cell_type":"code","source":"features.loc[2418, 'PoolQC'] = 'Fa'\nfeatures.loc[2501, 'PoolQC'] = 'Gd'\nfeatures.loc[2597, 'PoolQC'] = 'Fa'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('max_columns', None)\nfeatures[(features['GarageType'] == 'Detchd') & features['GarageYrBlt'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are houses with garages that are detached but that have NaN's for all other Garage variables. Let's impute these manually too."},{"metadata":{"trusted":true},"cell_type":"code","source":"features.loc[2124, 'GarageYrBlt'] = features['GarageYrBlt'].median()\nfeatures.loc[2574, 'GarageYrBlt'] = features['GarageYrBlt'].median()\n\nfeatures.loc[2124, 'GarageFinish'] = features['GarageFinish'].mode()[0]\nfeatures.loc[2574, 'GarageFinish'] = features['GarageFinish'].mode()[0]\n\nfeatures.loc[2574, 'GarageCars'] = features['GarageCars'].median()\n\nfeatures.loc[2124, 'GarageArea'] = features['GarageArea'].median()\nfeatures.loc[2574, 'GarageArea'] = features['GarageArea'].median()\n\nfeatures.loc[2124, 'GarageQual'] = features['GarageQual'].mode()[0]\nfeatures.loc[2574, 'GarageQual'] = features['GarageQual'].mode()[0]\n\nfeatures.loc[2124, 'GarageCond'] = features['GarageCond'].mode()[0]\nfeatures.loc[2574, 'GarageCond'] = features['GarageCond'].mode()[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the basements:\n\n* BsmtQual\n* BsmtCond\n* BsmtExposure\n* BsmtFinType1\n* BsmtFinType2\n* BsmtFinSF1\n* BsmtFinSF2\n* BsmtUnfSF\n* TotalBsmtSF"},{"metadata":{"trusted":true},"cell_type":"code","source":"basement_columns = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                   'BsmtFinType2', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n                   'TotalBsmtSF']\n\ntempdf = features[basement_columns]\ntempdfnulls = tempdf[tempdf.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select just the rows that have less then 5 NA's, meaning there is incongruency in the row.\ntempdfnulls[(tempdfnulls.isnull()).sum(axis=1) < 5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's impute all incongruencies with the most likely value"},{"metadata":{"trusted":true},"cell_type":"code","source":"features.loc[332, 'BsmtFinType2'] = 'ALQ' #since smaller than SF1\nfeatures.loc[947, 'BsmtExposure'] = 'No' \nfeatures.loc[1485, 'BsmtExposure'] = 'No'\nfeatures.loc[2038, 'BsmtCond'] = 'TA'\nfeatures.loc[2183, 'BsmtCond'] = 'TA'\nfeatures.loc[2215, 'BsmtQual'] = 'Po' #small basement so let's do Poor.\nfeatures.loc[2216, 'BsmtQual'] = 'Fa' #similar but a bit bigger.\nfeatures.loc[2346, 'BsmtExposure'] = 'No' #unfinished bsmt so prob not.\nfeatures.loc[2522, 'BsmtCond'] = 'Gd' #cause ALQ for bsmtfintype1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Zoning##"},{"metadata":{"trusted":true},"cell_type":"code","source":"subclass_group = features.groupby('MSSubClass')\nZoning_modes = subclass_group['MSZoning'].apply(lambda x : x.mode()[0])\nZoning_modes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the rest we will just use a loop to impute 'None' value."},{"metadata":{"trusted":true},"cell_type":"code","source":"objects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\n\nfeatures.update(features[objects].fillna('None'))\n\nnulls = np.sum(features.isnull())\nnullcols = nulls.loc[(nulls != 0)]\ndtypes = features.dtypes\ndtypes2 = dtypes.loc[(nulls != 0)]\ninfo = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\nprint(info)\nprint(\"There are\", len(nullcols), \"columns with missing values\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's think about imputing the missing values in the numerical features. Most of the time we impute 0, but sometimes something else is needed.\n\n* LotFrontage: This is linear feet of street connected to property. Let's impute with the median per neighborhood with the assumption that this is extremely linked to what kind of area you live in."},{"metadata":{"trusted":true},"cell_type":"code","source":"neighborhood_group = features.groupby('Neighborhood')\nlot_medians = neighborhood_group['LotFrontage'].median()\nlot_medians","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected the lotfrontage averages differ a lot per neighborhood so let's impute with the median per neighborhood."},{"metadata":{"trusted":true},"cell_type":"code","source":"features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also take a closer look at GarageYrBlt"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('max_columns', None)\nfeatures[(features['GarageYrBlt'].isnull()) & features['GarageArea'] > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GarageYrBlt does not have any incongruencies. Let's also examine MasVnrArea."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('max_columns', None)\nfeatures[(features['MasVnrArea'].isnull())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No incongruencies here either. The rest can be safely imputed with 0 since this means that the property is not present in the house."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filling in the rest of the NA's\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes: \n        numerics.append(i)\n        \nfeatures.update(features[numerics].fillna(0))\n\nnulls = np.sum(features.isnull())\nnullcols = nulls.loc[(nulls != 0)]\ndtypes = features.dtypes\ndtypes2 = dtypes.loc[(nulls != 0)]\ninfo = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\nprint(info)\nprint(\"There are\", len(nullcols), \"columns with missing values\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Incorrect values###\nSome values can be obviously wrong and this might impact our model. We used min and max values to check odd values in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"features.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the min and max of each variable there are some errors in the data.\n\n* GarageYrBlt - the max value is 2207, this is obviously wrong since the data is only until 2010.\n\nThe rest of the data looks fine. Let's inspect this row a bit more carefully and impute an approximate correct value."},{"metadata":{"trusted":true},"cell_type":"code","source":"features[features['GarageYrBlt'] == 2207]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This particular datapoint has YearBuilt in 2006 and YearRemodAdd in 2007. 2207 most likely is a data input error that should have been 2007 when the remodel happened. Let's impute 2007."},{"metadata":{"trusted":true},"cell_type":"code","source":"features.loc[2590, 'GarageYrBlt'] = 2007","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Factorization##\nThere are features that are read in as numericals but are actually objects. Let's transform them."},{"metadata":{"trusted":true},"cell_type":"code","source":"#factors = ['MSSubClass', 'MoSold']\nfactors = ['MSSubClass']\n \n\n\nfor i in factors:\n    features.update(features[i].astype('str'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Skew transformation features###\nLet's check skew in our features and transform if necessary."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import skew\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes: \n        numerics2.append(i)\n\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews = pd.DataFrame({'skew':skew_features})\nskews","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the boxcox1p transformation here because we tried the log transform first but a lot of skew remained in the data. Also using boxcox1p over normal boxcox because boxcox can't handle zero values."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nhigh_skew = skew_features[skew_features > 0.5]\nhigh_skew = high_skew\nskew_index = high_skew.index\n\nfor i in skew_index:\n    features[i]= boxcox1p(features[i], boxcox_normmax(features[i]+1))\n\n        \nskew_features2 = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews2 = pd.DataFrame({'skew':skew_features2})\nskews2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Incomplete cases###\nChecking to see if levels of variables in train and test datasets match and if not or if the level distribution is very low whether it should be deleted"},{"metadata":{"trusted":true},"cell_type":"code","source":"objects3 = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects3.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Set incomplete cases\")\n\nsums_features = features[objects3].apply(lambda x: len(np.unique(x)))\nsums_features.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a closer look at some of these lower numbered variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(features['Street'].value_counts())\nprint('-----')\nprint(features['Utilities'].value_counts())\nprint('-----')\nprint(features['CentralAir'].value_counts())\nprint('-----')\nprint(features['PavedDrive'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We experimented a bunch with this and decided in the end that if a column has low amount of levels and most values are in the same class (>97%) let's remove them.\n\nLet's delete Utilities because of how unbalanced it is."},{"metadata":{"trusted":true},"cell_type":"code","source":"#features = features.drop(['Utilities'], axis=1)\nfeatures = features.drop(['Utilities', 'Street'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating features##\nIn this section I create some features that can be created from the current data.\n\nSize of the house. There are a few variables dealing with square footage, I don't use TotalBsmtSF as a proxy for the basement because I believe unfinished square feet in the basement area won't have a big impact on price as it needs money to make it 'livable' square footage, so I just use BsmtSF1 and BsmtSF2.\n\n* BsmtFinSF1\n* BsmtFinSF2\n* 1stFlrSF\n* 2ndFlrSF\n\nAnother combined variable is the bathrooms in the house. I count fullbath for 1 and halfbath for 0.5.\n\n* FullBath\n* HalfBath\n* BsmtFullBath\n* BsmtHalfBath\n\nAnother combined variable is the total porch size.\n\n* OpenPorchSF\n* EnclosedPorch\n* 3SsnPorch\n* Screenporch\n* WoodDeckSF\n\nNext to that I make some simplified features.\n\n* haspool\n* has2ndfloor\n* hasgarage\n* hasbsmt\n* hasfireplace"},{"metadata":{"trusted":true},"cell_type":"code","source":"features['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5*features['HalfBath']) + \n                               features['BsmtFullBath'] + (0.5*features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                             features['WoodDeckSF'])\n\n\n#simplified features\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Dummies and Encode categorical features##\nSince sklearn lm.fit() does not accept strings we have to convert our objects to dummy variables. Also, Numerically encode categorical features because most models can only handle numerical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_features = pd.get_dummies(features).reset_index(drop=True)\nfinal_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recreate train and test sets\nNow we resplit the model in test and train"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = final_features.iloc[:len(y),:]\ntesting_features = final_features.iloc[len(X):,:]\n\nprint(X.shape)\nprint(testing_features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overfitting prevention###\n#### Outliers####\nLet's do a little bit more in-depth and rigorous analysis first on outliers. I'll employ Leave-One-Out methodology with OLS to find which points have a significant effect on our model fit."},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n\nols = sm.OLS(endog = y, exog = X)\n\nfit = ols.fit()\ntest2 = fit.outlier_test()['bonf(p)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers = list(test2[test2<1e-3].index) \n\noutliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we find that these are outliers. Let's delete these."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(X.index[outliers])\n\ny = y.drop(y.index[outliers])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dummy levels###\nTo prevent overfitting I'll also remove columns that have more than 97% 1 or 0 after doing pd.get_dummies."},{"metadata":{"trusted":true},"cell_type":"code","source":"overfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(X) * 100 >99.94:\n        overfit.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"overfit = list(overfit)\noverfit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop these from 'X' and 'testing_features'. Let's also drop MSZoning_C (all). It has about 99.44% zeros but unlike others with that kind of percentage it's being included in my lasso/ridge/elasticnet models with quite strong coefficient sizes."},{"metadata":{"trusted":true},"cell_type":"code","source":"overfit.append('MSZoning_C (all)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"overfit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.drop(overfit,axis=1,inplace=True)\ntesting_features.drop(overfit,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(testing_features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline model##\n### Full Model w/ kfold cross validation###\nLet's build a baseline linear regression model to benchmark our feature selected models and advanced models on.\n\nWe decided not to do a manual train/test split but instead rely completely on 10-fold cross-validation for every model including our benchmark.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\n\n#Build our model method\nlm = LinearRegression()\n\n#Build our cross validation method\nkfolds = KFold(n_splits=10, shuffle=True, random_state=23)\n\n#build our model scoring function\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, X, y, \n                                   scoring=\"neg_mean_squared_error\", \n                                   cv = kfolds))\n    return(rmse)\n\n\n#second scoring metric\ndef cv_rmsle(model):\n    rmsle = np.sqrt(np.log(-cross_val_score(model, X, y,\n                                           scoring = 'neg_mean_squared_error',\n                                           cv=kfolds)))\n    return(rmsle)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's fit our first model"},{"metadata":{"trusted":true},"cell_type":"code","source":"benchmark_model = make_pipeline(RobustScaler(),\n                                lm).fit(X=X, y=y)\ncv_rmse(benchmark_model).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing baseline model###\nLet's see how the residuals and predictions vs actual values are distributed. Here we should note this looks abit high. This gets reduced down with feature selection but the baseline model includes all which leads to a ton of multicollinearity causing high RMSE values.\n\n### Feature Selection##\nBefore starting this section it should be noted that we'll try to be extra careful not to create contamination during feature selection. Meaning that we'll select features constrained per fold in cross-validation to ensure no data leakage happens.\n\n### Filter methods###\n#### Coefficient importance####"},{"metadata":{"trusted":true},"cell_type":"code","source":"coeffs = pd.DataFrame(list(zip(X.columns, benchmark_model.steps[1][1].coef_)), columns=['Predictors', 'Coefficients'])\n\ncoeffs.sort_values(by='Coefficients', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This way of model fitting above is probably the simplest way to construct a machine learning model. However, Let's dive deep into some more complex regression. \n\n### Regularization Models\nWhat makes regression model more effective is its ability of *regularizing*. The term \"regularizing\" stands for models ability **to structurally prevent overfitting by imposing a penalty on the coefficients.** \n\n\nThere are three types of regularizations. \n* **Ridge**\n* **Lasso**\n* **Elastic Net**\n\nThese regularization methods work by penalizing **the magnitude of the coefficients of features** and at the same time **minimizing the error between the predicted value and actual observed values**.  This minimization becomes a balance between the error (the difference between the predicted value and observed value) and the size of the coefficients. The only difference between Ridge and Lasso is **the way they penalize the coefficients.** Elastic Net is the combination of these two. **Elastic Net** adds both the sum of the squares errors and the absolute value of the squared error. To get more in-depth of it, let us review the least squared loss function. \n\n**Ordinary least squared** loss function minimizes the residual sum of the square(RSS) to fit the data:\n\n### $$ \\text{minimize:}\\; RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 $$\n\nLet's review this equation once again, Here: \n* $y_i$ is the observed value. \n* $\\hat{y}_i$ is the predicted value. \n* The error = $y_i$ - $\\hat{y}_i$\n* The square of the error = $(y_i - \\hat{y}_i)^2$\n* The sum of the square of the error = $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$, that's the equation on the left. \n* The only difference between left sides equation vs. the right sides one above is the replacement of $\\hat{y}_i$, it is replaced by $\\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)$, which simply follow's the slope equation, y = mx+b, where, \n* $\\beta_0$ is the intercept. \n* **$\\beta_j$ is the coefficient of the feature($x_j$).**\n\nLet's describe the effect of regularization and then we will learn how we can use loss function in Ridge.\n* One of the benefits of regularization is that it deals with **multicollinearity**(high correlation between predictor variables) well, especially Ridge method. Lasso deals with **multicollinearity** more brutally by penalizing related coefficients and force them to become zero, hence removing them. However, **Lasso** is well suited for redundant variables. \n \n***\n<div>\n    \n   ### Ridge:\nRidge regression adds penalty equivalent to the square of the magnitude of the coefficients. This penalty is added to the least square loss function above and looks like this...\n\n### $$ \\text{minimize:}\\; RSS+Ridge = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n\nHere, \n* $\\lambda_2$ is constant; a regularization parameter. It is also known as $\\alpha$. The higher the value of this constant the more the impact in the loss function. \n    * When $\\lambda_2$ is 0, the loss funciton becomes same as simple linear regression. \n    * When $\\lambda_2$ is $\\infty$, the coefficients become 0\n    * When $\\lambda_2$ is between  0 and $\\infty$(0<$\\lambda_2$<$\\infty$), The $\\lambda_2$ parameter will decide the miagnitude given to the coefficients. The coefficients will be somewhere between 0 and ones for simple linear regression. \n* $\\sum_{j=1}^p \\beta_j^2$, is the squared sum of all coefficients. \n\nNow that we know every nitty-gritty details about this equation, let's use it for science, but before that a couple of things to remember. \n* It is essential to standardize the predictor variables before constructing the models. \n* It is important to check for multicollinearity,"},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression (L2 penalty)###"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\n\ndef ridge_selector(k):\n    ridge_model = make_pipeline(RobustScaler(),\n                                RidgeCV(alphas = [k],\n                                        cv=kfolds)).fit(X, y)\n    \n    ridge_rmse = cv_rmse(ridge_model).mean()\n    return(ridge_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_alphas = [.0001, .0003, .0005, .0007, .0009, \n          .01, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 20, 30, 50, 60, 70, 80]\n\nridge_scores = []\nfor alpha in r_alphas:\n    score = ridge_selector(alpha)\n    ridge_scores.append(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(r_alphas, ridge_scores, label='Ridge')\nplt.legend('center')\nplt.xlabel('alpha')\nplt.ylabel('score')\n\nridge_score_table = pd.DataFrame(ridge_scores, r_alphas, columns=['RMSE'])\nridge_score_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n\nridge_model2 = make_pipeline(RobustScaler(),\n                            RidgeCV(alphas = alphas_alt,\n                                    cv=kfolds)).fit(X, y)\n\ncv_rmse(ridge_model2).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model2.steps[1][1].alpha_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso:\nLasso adds penalty equivalent to the absolute value of the sum of coefficients. This penalty is added to the least square loss function and replaces the squared sum of coefficients from Ridge. \n\n## $$ \\text{minimize:}\\; RSS + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_1\\sum_{j=1}^p |\\beta_j|$$\n\nHere, \n* $\\lambda_2$ is a constant similar to the Ridge function. \n* $\\sum_{j=1}^p |\\beta_j|$ is the absolute sum of the coefficients."},{"metadata":{},"cell_type":"markdown","source":"### Lasso Regression (L1 penalty)###"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LassoCV\n\n\nalphas = [0.00005, 0.0001, 0.0003, 0.0005, 0.0007, \n          0.0009, 0.01]\nalphas2 = [0.00005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005,\n           0.0006, 0.0007, 0.0008]\n\n\nlasso_model2 = make_pipeline(RobustScaler(),\n                             LassoCV(max_iter=1e7,\n                                    alphas = alphas2,\n                                    random_state = 42)).fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = lasso_model2.steps[1][1].mse_path_\n\nplt.plot(alphas2, scores, label='Lasso')\nplt.legend(loc='center')\nplt.xlabel('alpha')\nplt.ylabel('RMSE')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_model2.steps[1][1].alpha_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_rmse(lasso_model2).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coeffs = pd.DataFrame(list(zip(X.columns, lasso_model2.steps[1][1].coef_)), columns=['Predictors', 'Coefficients'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"used_coeffs = coeffs[coeffs['Coefficients'] != 0].sort_values(by='Coefficients', ascending=False)\nprint(used_coeffs.shape)\nprint(used_coeffs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"used_coeffs_values = X[used_coeffs['Predictors']]\nused_coeffs_values.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"overfit_test2 = []\nfor i in used_coeffs_values.columns:\n    counts2 = used_coeffs_values[i].value_counts()\n    zeros2 = counts2.iloc[0]\n    if zeros2 / len(used_coeffs_values) * 100 > 99.5:\n        overfit_test2.append(i)\n        \noverfit_test2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Elastic Net: \nElastic Net is the combination of both Ridge and Lasso. It adds both the sum of squared coefficients and the absolute sum of the coefficients with the ordinary least square function. Let's look at the function. \n\n### $$ \\text{minimize:}\\; RSS + Ridge + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_1\\sum_{j=1}^p |\\beta_j| + \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n\nThis equation is pretty self-explanatory if you have been following this kernel so far."},{"metadata":{},"cell_type":"markdown","source":"### Elastic Net (L1 and L2 penalty)###\nOne of the issues with Lasso is that it's likely to pick, from correlated features, one at random. Elastic net would pick both. Its a bit of a mix between ridge and lasso. We decided to include it since R's implementation of ridge regression actually invovles some elasticNet properties."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNetCV\n\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nelastic_cv = make_pipeline(RobustScaler(), \n                           ElasticNetCV(max_iter=1e7, alphas=e_alphas, \n                                        cv=kfolds, l1_ratio=e_l1ratio))\n\nelastic_model3 = elastic_cv.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_rmse(elastic_model3).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(elastic_model3.steps[1][1].l1_ratio_)\nprint(elastic_model3.steps[1][1].alpha_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Xgboost##"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\n%matplotlib inline\nimport xgboost as xgb\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Belows function was used to obtain the optimal boosting rounds. This is accomplished useing xgb.cv's early stopping."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef modelfit(alg, dtrain, target, useTrainCV=True, \n             cv_folds=5, early_stopping_rounds=50):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain.values, \n                              label=y.values)\n        \n        print(\"\\nGetting Cross-validation result..\")\n        cvresult = xgb.cv(xgb_param, xgtrain, \n                          num_boost_round=alg.get_params()['n_estimators'], \n                          nfold=cv_folds,metrics='rmse', \n                          early_stopping_rounds=early_stopping_rounds,\n                          verbose_eval = True)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    print(\"\\nFitting algorithm to data...\")\n    alg.fit(dtrain, target, eval_metric='rmse')\n        \n    #Predict training set:\n    print(\"\\nPredicting from training data...\")\n    dtrain_predictions = alg.predict(dtrain)\n        \n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"RMSE : %.4g\" % np.sqrt(mean_squared_error(target.values,\n                                             dtrain_predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gridsearching gave me optimal parameters for XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb3 = XGBRegressor(learning_rate =0.01, n_estimators=3460, max_depth=3,\n                     min_child_weight=0 ,gamma=0, subsample=0.7,\n                     colsample_bytree=0.7,objective= 'reg:linear',\n                     nthread=4,scale_pos_weight=1,seed=27, reg_alpha=0.00006)\n\nxgb_fit = xgb3.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM##"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\nlgbm_model = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_rmse(lgbm_model).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_fit = lgbm_model.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble methods##\nLet's see if we can get a better performance on the test data by employing ensemble methods. To stay in the constraints of the exercise, we combine these models.\n\n* LassoCV\n* RidgeCV\n* Elasticnet\n* Xgboost\n* Lightgbm\n\nExperimenting with averaging takes a lot of time. Basically we're optimizing the tradeoff between under and over fitting.\n\nFirst we build a meta-regressor through a process called **stacking generalization** which trains a model on a part of the training set (it gets split first into a new training set and a holdout set). Then the algorithm tests these models on the holdout set and uses these predictions (called out-of-fold predictions) as input for the 'meta model'. Below is a grahpical representation of the process."},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nfrom IPython.core.display import HTML \nImage(url = \"http://i.imgur.com/QBuDOjs.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensemble 1 - Stacking Generalization###\nTo try to seek out more performance of our rank let's try Stacking Generalization, using the mlxtend package to implement it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline\n\n#setup models\nridge = make_pipeline(RobustScaler(), \n                      RidgeCV(alphas = alphas_alt, cv=kfolds))\n\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas = alphas2,\n                              random_state = 42, cv=kfolds))\n\nelasticnet = make_pipeline(RobustScaler(), \n                           ElasticNetCV(max_iter=1e7, alphas=e_alphas, \n                                        cv=kfolds, l1_ratio=e_l1ratio))\n\nlightgbm = make_pipeline(RobustScaler(),\n                        LGBMRegressor(objective='regression',num_leaves=5,\n                                      learning_rate=0.05, n_estimators=720,\n                                      max_bin = 55, bagging_fraction = 0.8,\n                                      bagging_freq = 5, feature_fraction = 0.2319,\n                                      feature_fraction_seed=9, bagging_seed=9,\n                                      min_data_in_leaf =6, \n                                      min_sum_hessian_in_leaf = 11))\n\nxgboost = make_pipeline(RobustScaler(),\n                        XGBRegressor(learning_rate =0.01, n_estimators=3460, \n                                     max_depth=3,min_child_weight=0 ,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective= 'reg:linear',nthread=4,\n                                     scale_pos_weight=1,seed=27, \n                                     reg_alpha=0.00006))\n\n\n#stack\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, \n                                            xgboost, lightgbm), \n                               meta_regressor=xgboost,\n                               use_features_in_secondary=True)\n\n#prepare dataframes\nstackX = np.array(X)\nstacky = np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scoring \n\nprint(\"cross validated scores\")\n\nfor model, label in zip([ridge, lasso, elasticnet, xgboost, lightgbm, stack_gen],\n                     ['RidgeCV', 'LassoCV', 'ElasticNetCV', 'xgboost', 'lightgbm',\n                      'StackingCVRegressor']):\n    \n    SG_scores = cross_val_score(model, stackX, stacky, cv=kfolds,\n                               scoring='neg_mean_squared_error')\n    print(\"RMSE\", np.sqrt(-SG_scores.mean()), \"SD\", scores.std(), label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_gen_model = stack_gen.fit(stackX, stacky)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensemble 2 - averaging###\nFinal averaging weights are mostly trial and error "},{"metadata":{"trusted":true},"cell_type":"code","source":"em_preds = elastic_model3.predict(testing_features)\nlasso_preds = lasso_model2.predict(testing_features)\nridge_preds = ridge_model2.predict(testing_features)\nstack_gen_preds = stack_gen_model.predict(testing_features)\nxgb_preds = xgb_fit.predict(testing_features)\nlgbm_preds = lgbm_fit.predict(testing_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Blend models in order to make the final predictions more robust to overfitting###"},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_preds = ((0.2*em_preds) + (0.1*lasso_preds) + (0.1*ridge_preds) + \n               (0.2*xgb_preds) + (0.1*lgbm_preds) + (0.3*stack_gen_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Actual predictions for Kaggle###\nLet's transform the predictions back to normal values because the model is trained with logSalePrice."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/group-assignment-ace-2020/sample_submission.csv\")\nsubmission.iloc[:,1] = np.expm1(stack_preds)\nsubmission.to_csv(\"final_submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}